# Treemap Comparison Resources
This repository hosts the resources used in the paper **Quantitative Comparison of Time-Dependent Treemaps**, by Eduardo Vernier, Max Sondag, Joao Comba, Bettina Speckmann, Alexandru Telea, and Kevin Verbeek.

The `./Datasets` directory contains the dynamic hierarchy datasets used in the benchmark and the code that generated them. Note that there are more datasets than shown in the paper.

`./Treemapping Code` has the Java implementations of Approximate (APP), Hilbert (HIL), Local Moves using zero (LM0) or four (LM4) local moves, Moore (MOO), Ordered with Pivot-by-Middle (PBM), Pivot-by-Size (PBZ), and Pivot-by-Split (PBS),
Slice-and-Dice (SND), Spiral (SPI), Split (SPI), Squarified (SQR), and Strip (STR).

`./Metrics and Plots Code` has the Python code that computes the metric results for the treemap layouts and generates the plots seen in the paper. 

`./Figures` contains the plots for the 46 representatives datasets in the paper.

## Generating Treemaps
This project contains all the code to generate the treemap layouts for 13 different algorithms, visualize them and classify the datasets. Datasets can be classified using `DatasetClassifier.DataSetClassifier.java` as the main class. A folder can be specified here from where to read the data from as well as where to output the classification.

Layouts can be generated by running `UserControl.Simulator.java`. This method can be be called from the commandline.
An example commandline run is:
`-width 1000 -height 1000 -technique moore -inputfolder D:/datasets/ -outputfolder D:/output/ -baseline true`

In case -baseline is set to true, baselines will be generated for each algorithm as well.
In the function `getTreeMapGenerators` one can specify for which algorithms to run the simulation and generate the output.

If one want to generate baselines for an algorithm not implemented in this codebase, they can use the class `statistics.Baseline.BaseLineGenerator`
In the main method one can specify where their folder with layouts and dataset is contained, and where to output the generated baselines
In case one want to visualize the datasets using a certain algorithm, `UserControl.Visualiser.Visualizer.java` should be chosen as the main class. One can add additional datasets to be selected in `GUI.java`.

## Generating the figures
To generate the figures it is first necessary to pre-compute the quality metrics. 
From the `./Metrics and Plots Code/` directory, run the command `python3 Main.py cache-metrics <dataset_id>`, where dataset_id is the name of the folder that the real layouts are stored, e.g. 'exo' or 'Hystrix', we compute the Aspect Ratio, Corner Travel (real and baseline), and Relative Position Change (real and baseline) metrics cell-wise for all revisions and techniques.

Following the order of the paper, to generate a real vs. baseline KDE scatter plot for Corner Travel and Relative Position Change for a given dataset, run `python3 Main.py kde-ct <dataset_id>` and `python3 Main.py kde-rpc <dataset_id>`. 

To generate the time boxplots as seen in Figure 7, run `python3 Main.py boxplots <dataset_id>`. Results for the 4 measurements (unweighted aspect ratio, weighted aspect ratio, diff corner travel, and diff relative position change) will be generated.

To generate Figures 8 (Matrix) and 9 (Star-glyph scatterplot), run `python3 Main.py matrix <dataset_id list>` and `python3 Main.py scatter <dataset_id list>`, where dataset_ids are separated by a single white space, e.g., `python3 Main.py matrix gh-keras-m tmbd-yearly-count-genres-children MoviesHC9Y1W`. The filtered version are generated in the `Notebooks/FilteredStarPlot-newcenters.ipynb` file.

The Rank Matrix (Fig 10) is generated nunnig the `Notebooks/RankMatrix.ipynb` file.

## Dataset extraction
#### GitHub
https://github.com/EduardoVernier/dataset-generator/tree/master/GitHubMostWanted
The dataset selection was done through the scrapping of the first 10 pages of  http://gitmostwanted.com/, which is a website that lists popular github repositories. 
For the selected URLs, I ran a python script that clones revisions of a repository with a given periodicity. For this batch, I did monthly extractions (-m), but the script takes a flag as argument that selects if it should clone yearly (-y), daily (-d), or all revisions (-a).
For each cloned revision, the script invokes the CLOC [link] tool to list and count the number of lines of code in each source file. This is a faster free alternative to Understand.
Datasets are named as gh-<name_of_repository>-<periodicity>. 
Ex.: gh-svgo-m for the github.com/svg/svgo repository with last commit of month extraction.
The scripts and notebooks for scrapping, metric collection and file generation can be found at the link above. A text file with all the repository URLs is also available.

#### WorldBank
https://github.com/EduardoVernier/dataset-generator/tree/master/WorldBank
The WorldBank Group makes available a large csv file with data from all their indicators [link]. Based on that, each indicator that has a minimum number of observations is turned into a dataset. Each row represents one Region/Country entry and the columns give the observation value for that entry in a given year. There is one column for each year since the first time the indicator was measured. Each indicator has a unique id, e.g., wb-TX.VAL.MMTL.ZS.UN corresponds to "Ores and metals exports (% of merchandise exports)" and wb-EN.URB.MCTY.TL.ZS corresponds to "Population in urban agglomerations of more than 1 million (% of total population)". A table with the mappings and meta information can be found at the wb-series.csv file in the link above.
Dataset are named wb-<indicator_id>. Ex.: wb-AG.PRD.FOOD.XD

#### TMDB
https://github.com/EduardoVernier/dataset-generator/tree/master/TMDB
The dataset contains over 20 million reviews about 27,278 movies.
Each user review of a movie has a rating (value from 0 to 5) and a timestamp of when it was posted. The first reviews date to January 1995 and the last to March 2015. 
Each movie has an id, year of release and a list of genres it belongs to (sorted alphabetically).
To extract 100+ datasets from this source I defined 4 levels of freedom. In bold are the keys that are used to identify the datasets.

1. Hierarchy
1.1. Using genre information. 0 to 7 levels (some movies don't have this info)   [genre]
Ex.: Adventure/Animation/Children/Comedy/ToyStory, Comedy/Drama/Romance/War/ForrestGump
1.2. Using year information. Always two levels.  [year]
            Ex.:  1995/ToyStory, 1994/ForrestGump.

2. Cell weight
2.1. Number of reviews in a given period.  [count]
2.2. Average rating in a given period.  [average]
2.3. Review standard deviation in a given period. [std]
    
3. Time aggregation by review timestamp
3.1. Aggregate monthly (~240 months or revisions)   [monthly]
3.2. Aggregate yearly (~20 years or revisions)   [yearly]
    
4. Filters
4.1. Only action movies.  [action]
4.2. Only children movies.  [children]
4.3. Only documentaries.  [documentary]
4.4. Only movies from 60s, 70s and 80's.  [60sto80s]
4.5. Only movies from 90s.  [90s]
4.6. Only movies from 2000 onwards. [00stonow]  
4.7. Only movies with 4 or more genres.  [4plusngeres]
4.8. Only movies without genre info.  [nogenre]
4.9. 2000 randomly selected movies.  [2krand]

This gives us 2 * 3 * 2 * 9 = 108 datasets. Datasets are named tmdb-<periodicity>-<weight_value>-<hierarchy_type>-<filter> .
For example:
tmbd-monthly-mean-genres-documentaries gives the average monthly rating for documentaries using the genre hierarchy and tmbd-yearly-count-release-90s counts the number of reviews 90s titles received during each year using the hierarchy given by Year/Title.

#### Movielens
The MovieLens database. This database contains 45,000 movies, 750,000 keywords attached to these movies, and 26 million time-stamped 0 to 5 star ratings over roughly 22 years. The naming convention for these files is as follows:

_Movies{Hierachical}{Cumulative}{collection period}{aggregation period}_

If the name contains the H of _{Hierarchical}_ we have a fixed 4 level hierachy.
We first partitioned on the genres Crime, Adventure and Drama; second on the movie release date (before vs after 2010); third on the tags Ford, Pitt, Depp, Hanks, Stewart, Cooper, Grant, and Flynn; fourth on whether the movie title contains the word Act, War, Love, Time, Spirit, Night, or any other word; fifth on whether tags contain Friend, Enemy or neither/both; and finally on whether tags contain Past, Future or neither/both. This results in a very deep hierarchy with a large imbalance in the tree depths.

If the name does not contains the H of _{Hierarchical}_ it is a dataset consisting of only a single level. In this case we only considered movies with genres Action and Adventure to trim down the sheer amount of movies.

If the name contains the C of _{Cumulative}_ it means we consider all ratings from the start of the collection period untill now. Otherwise we only consider the ratings in the current aggregation period.

The _{collection period}_ indicates over how long a period we are collecting the data. The _{aggregation period}_ indicates how much time passes between two samples. Y indicates a year, M indicates a month, D indicates a day, H indicates an hour.

